{
 "metadata": {
  "name": "",
  "signature": "sha256:974a75e3adc99fb6024510a56d5b93f6f24ba081299b7c9b2931c47cac7f697f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Cluster Crowd Tags\n",
      "Cluster tags annotated by crowd, and replace the original tags with the clustered version."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd 'C:\\Users\\IBM_ADMIN\\Google Drive\\Crowd-WATSON\\Research\\CROWDSOURCING EXPERIMENTS\\Sound Annotations (Benjamin_Emiel)\\processing'\n",
      "\n",
      "# Standard\n",
      "from math import log\n",
      "from collections import defaultdict, Counter\n",
      "from itertools import combinations,product,permutations\n",
      "\n",
      "# Non-standard\n",
      "import unicodecsv\n",
      "import os\n",
      "import requests\n",
      "from tabulate import tabulate\n",
      "from gensim.models import Word2Vec\n",
      "import networkx as nx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Users\\IBM_ADMIN\\Google Drive\\Crowd-WATSON\\Research\\CROWDSOURCING EXPERIMENTS\\Sound Annotations (Benjamin_Emiel)\\processing\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"load vector space model (this takes a few minutes)\"\n",
      "\n",
      "from gensim.models import Word2Vec\n",
      "global model\n",
      "global vocab\n",
      "googlenews = 'c:/GoogleNews-vectors-negative300.bin.gz'\n",
      "model      = Word2Vec.load_word2vec_format(googlenews, binary=True)\n",
      "vocab      = set(model.vocab.keys())\n",
      "print \"Done loading GoogleNews vector space model.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "################################################################################\n",
      "# I/O\n",
      "\n",
      "def crowd_dict(filename):\n",
      "    \"Create a dictionary: k=id, v=list of tags associated with the id.\"\n",
      "  \n",
      "    # Open the file:\n",
      "    with open(filename) as f:\n",
      "        reader = unicodecsv.reader(f)\n",
      "        reader.next() # skip the top row, which contains the header.\n",
      "        d = {}\n",
      "        for row in reader:\n",
      "            if row[42] not in d:\n",
      "                d[row[42]] = []\n",
      "            tags = row[35].lower().strip().split(',')\n",
      "            for tag in tags:\n",
      "                tag = tag.strip()\n",
      "                if len(tag) > 0:\n",
      "                    d[row[42]].append(tag)\n",
      "        return d\n",
      "\n",
      "def freesound_dict():\n",
      "    data = {}\n",
      "    with open(\"data/all_tags.txt\") as f:\n",
      "        for line in f:\n",
      "            split = line.split()\n",
      "            data[split[0]] = split[1:]\n",
      "    return data\n",
      "\n",
      "################################################################################\n",
      "# Stats\n",
      "\n",
      "def log_likelihood(corpus_1_counts,corpus_2_counts):\n",
      "    \"\"\"Computes the log likelihood for words to appear more in one corpus than\n",
      "    in the other.\"\"\"\n",
      "    # Rayson, P. and Garside, R. (2000)\n",
      "    # http://dl.acm.org/citation.cfm?id=1117730\n",
      "    total_c1    = float(sum(corpus_1_counts.values()))\n",
      "    total_c2    = float(sum(corpus_2_counts.values()))\n",
      "    total       = total_c1 + total_c2\n",
      "    overlap     = set(corpus_1_counts.keys()) & set(corpus_2_counts.keys())\n",
      "    l = []\n",
      "    for k in overlap:\n",
      "        a = float(corpus_1_counts[k])\n",
      "        b = float(corpus_2_counts[k])\n",
      "        E1 = total_c1 * (a+b) / total\n",
      "        E2 = total_c2 * (a+b) / total\n",
      "        LL = 2 * ((a * log(a/E1)) + (b * log(b/E2)))\n",
      "        l.append((LL,k))\n",
      "    return sorted(l,reverse=True)\n",
      "\n",
      "def typical_words(corpus_1_counts,corpus_2_counts,c1_name='c1',c2_name='c2'):\n",
      "    \"\"\"Uses the log likelihood function to determine words that are typical for\n",
      "    the first corpus (c1) and the second corpus (c2).\"\"\"\n",
      "    \n",
      "    # Thanks to Paul Rayson's website (http://ucrel.lancs.ac.uk/llwizard.html)\n",
      "    # we are able to say something about which words occur *significantly* more\n",
      "    # in one corpus than in the other. See the other note below.\n",
      "    \n",
      "    sorted_log_scores = log_likelihood(corpus_1_counts,corpus_2_counts)\n",
      "    \n",
      "    total       = sum(corpus_1_counts.values())\n",
      "    c1_rel_freq = dict((k,float(val)/total) for k,val in corpus_1_counts.items())\n",
      "    \n",
      "    total       = sum(corpus_2_counts.values())\n",
      "    c2_rel_freq = dict((k,float(val)/total) for k,val in corpus_2_counts.items())\n",
      "    \n",
      "    c1_words = []\n",
      "    c2_words = []\n",
      "    for val,word in sorted_log_scores:\n",
      "        if not val > 3.84:\n",
      "            # this corresponds to the 0.05 significance level according to:\n",
      "            # http://ucrel.lancs.ac.uk/llwizard.html\n",
      "            break\n",
      "        if c1_rel_freq[word] > c2_rel_freq[word]:\n",
      "            c1_words.append(word)\n",
      "        elif c1_rel_freq[word] < c2_rel_freq[word]:\n",
      "            c2_words.append(word)\n",
      "    return {c1_name:c1_words, c2_name:c2_words}\n",
      "\n",
      "def spacetodash(l):\n",
      "    \"Replace spaces by dashes to ease the comparison.\"\n",
      "    return [w.replace(' ','-') for w in l]\n",
      "\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# Cleaner decorator\n",
      "\n",
      "def cleaner(func):\n",
      "    \"Clean the pair lists that are produced by the cleaning functions below.\"\n",
      "    \n",
      "    def clean_pairlist(*args, **kwargs):\n",
      "    # If something needs to be done with the output of the cleaning functions,\n",
      "    # put it here and it will apply to all of them.\n",
      "    \n",
      "    # This code prevents strings from mapping to multiple strings.\n",
      "    # It chooses the most frequent target string.\n",
      "        pairs   = set(func(*args, **kwargs))\n",
      "        d       = defaultdict(list)\n",
      "        second  = [b for a,b in pairs]\n",
      "        for a,b in pairs:\n",
      "            d[a].append(b)\n",
      "        return set([(a,max(d[a], key=lambda b:second.count(b))) for a in d])\n",
      "    return clean_pairlist\n",
      "\n",
      "################################################################################\n",
      "# Some useful cleaning functions.\n",
      "\n",
      "def levenshtein(s, t):\n",
      "        ''' From Wikipedia article; Iterative with two matrix rows. '''\n",
      "        if s == t: return 0\n",
      "        elif len(s) == 0: return len(t)\n",
      "        elif len(t) == 0: return len(s)\n",
      "        v0 = [None] * (len(t) + 1)\n",
      "        v1 = [None] * (len(t) + 1)\n",
      "        for i in range(len(v0)):\n",
      "            v0[i] = i\n",
      "        for i in range(len(s)):\n",
      "            v1[0] = i + 1\n",
      "            for j in range(len(t)):\n",
      "                cost = 0 if s[i] == t[j] else 1\n",
      "                v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n",
      "            for j in range(len(v0)):\n",
      "                v0[j] = v1[j]\n",
      " \n",
      "        return v1[len(t)]\n",
      "\n",
      "def clean(l,check_function):\n",
      "    \"\"\"General function that takes a list of tags-to-be-cleaned and a cleaning\n",
      "    function (defined below) and returns a cleaned list.\"\"\"\n",
      "    d = dict(check_function(l))\n",
      "    for k in set(l):\n",
      "        if not k in d.keys():\n",
      "            d[k] = k\n",
      "    cleaned_list = [d[i] for i in l]\n",
      "    return cleaned_list, d.items()\n",
      "\n",
      "def clean_all(d,check_function,replacements=None):\n",
      "    \"Cleans all the entries in the dictionary using a particular function.\"\n",
      "    # Initialize:\n",
      "    num_items          = float(len(d.keys()))\n",
      "    new_index          = dict()\n",
      "\n",
      "    # For each sound, and its associated list of tags:\n",
      "    for sound_id,tags in d.items():\n",
      "        # Compute stats before, then initialize some variables for the loop.\n",
      "        old_tags = []\n",
      "        new_tags = tags\n",
      "        counter  = 0\n",
      "        # Clean the list until a next round of cleaning doesn't yield any difference.\n",
      "        # Maximum number of rounds: 10. Else we assume there's an infinite loop.\n",
      "        # I don't expect this to happen, but it's best to be safe. We'll get a notification\n",
      "        # if this happens.\n",
      "        while old_tags != new_tags:\n",
      "            old_tags    = new_tags\n",
      "            new_tags,changes = clean(old_tags,check_function)\n",
      "            replacements[sound_id].update(changes)\n",
      "            counter    += 1\n",
      "            if counter > 10:\n",
      "                print \"Infinite loop? I'm out!\"\n",
      "                break\n",
      "        # Update the tags for the sound, and add length of the list after cleaning\n",
      "        # to the stats.\n",
      "        new_index[sound_id] = new_tags\n",
      "    # Print the stats for this round, and return new index + all replacement data.\n",
      "    return new_index, replacements\n",
      "\n",
      "def changed(d,check_function):\n",
      "    \"Returns the keys of the items for which changes are proposed.\"\n",
      "    return [k for k in d if not check_function(d[k]) == set([])]\n",
      "\n",
      "################################################################################\n",
      "# Syntactic Cleaning\n",
      "# - order\n",
      "# - misspelling\n",
      "# - spaces\n",
      "# - morphology\n",
      "\n",
      "# Same problems as in phonological theory: how do you order these functions?\n",
      "# A function may make the list more suitable for another function, so that it can\n",
      "# reduce it further (FEEDING), or it may actually change the data so that another\n",
      "# function cannot reduce the list anymore (BLEEDING). Reference:\n",
      "#\n",
      "# Kiparsky, Paul. 1968. Linguistic universals and linguistic change. In Universal in Linguistic\n",
      "# Theory, ed. Emmon Bach and Robert T. Harms, 170-202. New York: Holt, Reinhart, and Winston.\n",
      "\n",
      "@cleaner\n",
      "def check_dashes(l):\n",
      "    \"Normalizes strings by replacing all dashes by spaces.\"\n",
      "    return [(w,w.replace('-',' ')) for w in l]\n",
      "\n",
      "@cleaner\n",
      "def check_order(l):\n",
      "    \"\"\"Eliminates order variation in multiword expressions, by taking all order-\n",
      "    variants and mapping them to a string where all words are sorted alphabetically.\"\"\"\n",
      "    pairs = []\n",
      "    phrases = [w.split() for w in set(l) if ' ' in w]\n",
      "    for a,b in combinations(phrases,2):\n",
      "        if set(a) == set(b):\n",
      "            a = ' '.join(sorted(a))\n",
      "            b = ' '.join(b)\n",
      "            pairs.append((a,a))\n",
      "            pairs.append((b,a))\n",
      "    return pairs\n",
      "\n",
      "@cleaner\n",
      "def check_spaces(l):\n",
      "    \"Proposes pairs (a,b) where a is equal to b without a space.\"\n",
      "    return [(w,''.join(w.split())) for w in l if ' ' in w\n",
      "                                              and ''.join(w.split()) in l]\n",
      "\n",
      "@cleaner\n",
      "def check_inclusion(l):\n",
      "    \"Proposes pairs (a,b) where b contains a space and a is included in b.\"\n",
      "    words       = set(l)\n",
      "    space_words = set([w for w in l if ' ' in w])\n",
      "    return [(w,sw) for sw in space_words for w in words & set(sw.split(' '))]\n",
      "\n",
      "@cleaner\n",
      "def check_substring(l):\n",
      "    \"Proposes pairs (a,b) where b contains a space and a is a substring of b.\"\n",
      "    words       = set(l)\n",
      "    space_words = set([w for w in l if ' ' in w])\n",
      "    nospace     = words - space_words\n",
      "    return [(w,sw) for w,sw in product(nospace,space_words) if w in sw]\n",
      "\n",
      "@cleaner\n",
      "def check_spelling(l,threshold=1):\n",
      "    \"\"\"Proposes pairs (a,b) where b is the spell-corrected version of a.\n",
      "    \n",
      "    This function makes use of an extensive word list. If a word A is not in the\n",
      "    list and it has a smaller Levenshtein distance to another word B in l than\n",
      "    the threshold, then we assume that B is the correct spelling of A.\"\"\"\n",
      "    pairs = []\n",
      "    for a,b in combinations(l,2):\n",
      "        if len(a) > 3 and len(b) > 3:\n",
      "            if a in vocab and not b in vocab:\n",
      "                if levenshtein(a,b) <= threshold:\n",
      "                    pairs.append((b,a))\n",
      "            elif b in vocab and not a in vocab:\n",
      "                if levenshtein(a,b) <= threshold:\n",
      "                    pairs.append((a,b))\n",
      "    return pairs\n",
      "\n",
      "@cleaner\n",
      "def check_morphology(l):\n",
      "    \"\"\"Proposes reductions of words to their stem if the stem or another\n",
      "    morphological variant is also present in l\"\"\"\n",
      "    tags  = set(l)\n",
      "    pairs = []\n",
      "    for w in l:\n",
      "        if w.endswith('s'):\n",
      "            stem     = w[:-1]\n",
      "            variants = set([stem + suffix for suffix in ['','ed','ing']])\n",
      "            if not variants.isdisjoint(tags):\n",
      "                pairs.append((w,stem))\n",
      "        elif w.endswith('ed'):\n",
      "            stem     = w[:-2]\n",
      "            variants = set([stem + suffix for suffix in ['','s','ing']])\n",
      "            if not variants.isdisjoint(tags):\n",
      "                pairs.append((w,stem))\n",
      "        elif w.endswith('ing'):\n",
      "            stem = w[:-3]\n",
      "            variants = set([stem + suffix for suffix in ['','ed','s']])\n",
      "            if not variants.isdisjoint(tags):\n",
      "                pairs.append((w,stem))\n",
      "    return pairs\n",
      "\n",
      "################################################################################\n",
      "# Semantic Cleaning\n",
      "\n",
      "@cleaner\n",
      "def check_semantics(l,threshold=0.75):\n",
      "    \"Proposes pairs of semantically related words.\"\n",
      "    return [tuple(sorted([a,b],key=lambda x:l.count(x)))\n",
      "                for a,b in combinations(set(l),2)\n",
      "                if a in vocab and b in vocab\n",
      "                and model.similarity(a,b) >= threshold]\n",
      "\n",
      "################################################################################\n",
      "# Shorthand functions to test everything.\n",
      "\n",
      "def run_cleaning(filename, clean_functions = [ check_dashes,\n",
      "                                     check_spaces,\n",
      "                                     check_spelling,\n",
      "                                     check_order,\n",
      "                                     check_morphology,\n",
      "                                     check_inclusion,\n",
      "                                     check_semantics,\n",
      "                                     check_substring   ]):\n",
      "    \"Wrapper to quickly test the result of the various cleaning functions.\"\n",
      "    try:\n",
      "        'test' in vocab\n",
      "    except:\n",
      "        print \"Please load the vector space model using load_model\"\n",
      "        return None\n",
      "    cd           = crowd_dict(filename)\n",
      "    d            = cd.copy()\n",
      "    sound_ids    = d.keys()\n",
      "    replacements = {k:set() for k in sound_ids}\n",
      "    \n",
      "    \n",
      "    #print sum(length_before)/num_items, \"was reduced to\", sum(length_after)/num_items\n",
      "    \n",
      "    for func in clean_functions:\n",
      "        d, replacements = clean_all(d,func,replacements)\n",
      "    graph_dict = dict()\n",
      "    for k in sound_ids:\n",
      "        G = nx.DiGraph()\n",
      "        G.add_edges_from((b,a) for a,b in replacements[k])\n",
      "        graph_dict[k] = G\n",
      "\n",
      "    clusters = {k:\n",
      "               {desc: tag\n",
      "               for tag in set(d[k]) \n",
      "               for desc in nx.descendants(graph_dict[k],tag)|set([tag]) if tag is not desc}\n",
      "               for k in d}\n",
      "\n",
      "    return d, clusters\n",
      "\n",
      "# replace the tags with their clustered variant\n",
      "def replaceTags(filename):\n",
      "    d,clusters = run_cleaning('1-input-crowd/'+filename)\n",
      "    keys = d.keys()\n",
      "   \n",
      "    f = open('1-input-crowd/'+filename, 'r')\n",
      "    w = open('2-clustered/'+filename, 'wb')\n",
      "    reader = unicodecsv.reader(f)\n",
      "    wr = unicodecsv.writer(w)\n",
      "\n",
      "    # write column names\n",
      "    wr.writerow(reader.next())\n",
      "\n",
      "    for row in reader:\n",
      "        old = row[35]\n",
      "        tags = row[35].lower().strip().split(',')\n",
      "        newtags = []\n",
      "        for tag in tags:\n",
      "            tag = tag.strip()\n",
      "            if len(tag) > 0:\n",
      "                if tag in clusters[row[42]]:\n",
      "                    tag = clusters[row[42]][tag]\n",
      "                newtags.append(tag)\n",
      "        row[35] = \",\".join(newtags)\n",
      "        #rint old,\"->\",row[35]\n",
      "        wr.writerow(row)\n",
      "    \n",
      "    f.close()\n",
      "    w.close()\n",
      "        \n",
      "    print \"Processed\",filename\n",
      "\n",
      "# replace tags in all crowd files\n",
      "files = os.listdir('1-input-crowd')\n",
      "for filename in files:\n",
      "    replaceTags(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Run CrowdTruth metrics\n",
      "The crowdsourcing results are loaded into CrowdTruth, after which the metrics run for each file. This process make take some time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save list of job id's on CrowdTruth\n",
      "jobs = []\n",
      "\n",
      "for f in os.listdir('2-clustered/')[1:]:\n",
      "    files = {'file': open('2-clustered/'+f, 'rb')}\n",
      "    values = {'input-project':'sounds',\n",
      "              'input-type':'sound',\n",
      "              'output-type':'sound2'}\n",
      "    r = requests.post('http://localhost/api/import/importresults', files=files, data=values)\n",
      "    print f,r.text\n",
      "    # break if there was an error\n",
      "    if not r.text.startswith('entity/sounds/job/'):\n",
      "        break\n",
      "    # on success, add job id to list of completed jobs\n",
      "    jobs += r.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2-1-150-f680552.csv localhost:27017: insertDocument :: caused by :: 11000 E11000 duplicate key error index: CrowdTruth_DEV.activities.$_id_  dup key: { : \"activity/metrics/2\" }done\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. Export sound and worker results to XML\n",
      "The metrics for all sounds and crowd workers are exported from CrowdTruth and saved as XML"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree as et\n",
      "\n",
      "# temp: make list of job id's\n",
      "jobs = [1,2]\n",
      "print jobs\n",
      "\n",
      "results = {}\n",
      "\n",
      "# for each job get the units and workers\n",
      "for job in jobs:\n",
      "    r = requests.get('http://localhost/api/analytics/job?job=entity/sounds/job/'+str(job))\n",
      "    results['entity/sounds/job/'+str(job)] = r.json()['infoStat']\n",
      "\n",
      "\n",
      "xml = et.Element(\"soundcollection\")    \n",
      "\n",
      "for job in results:\n",
      "    \n",
      "    # open the file of this job\n",
      "    filename = results[job]['platformJobId'] + '.csv'\n",
      "    workers = results[job]['metrics']['workers']['withoutFilter']\n",
      "    \n",
      "    # open the files to combine the original input\n",
      "    # build index because some files have extra columns\n",
      "    r = open('1-input-crowd/'+filename, 'r')\n",
      "    raw = unicodecsv.reader(r)\n",
      "    ri = raw.next()\n",
      "    ri = {i:ri.index(i) for i in ri}\n",
      "    \n",
      "    c = open('2-clustered/'+filename, 'r')\n",
      "    clustered = unicodecsv.reader(c)\n",
      "    ci = clustered.next()\n",
      "    ci = {i:ci.index(i) for i in ci}\n",
      "   \n",
      "    # add all sounds\n",
      "    for sound in clustered:\n",
      "        if xml.find(\".//sound[@id='\"+sound[ci['id']]+\"']\") is None:\n",
      "            s = et.SubElement(xml, \"sound\")\n",
      "            s.set('id', sound[ci['id']])\n",
      "            s.set('name', sound[ci['name']])\n",
      "            s.set('url', sound[ci['url']])\n",
      "            s.set('type', sound[ci['type']])\n",
      "            s.set('samplerate', sound[ci['samplerate']])\n",
      "            s.set('license', sound[ci['license']])\n",
      "            s.set('duration', sound[ci['duration']])\n",
      "            s.set('channels', sound[ci['channels']])\n",
      "            s.set('bitrate', sound[ci['bitrate']])\n",
      "            s.set('bitdepth', sound[ci['bitdepth']])\n",
      "            \n",
      "            descr = et.SubElement(s, \"description\").text = sound[ci['description']]\n",
      "            ratings = et.SubElement(s, \"ratings\")\n",
      "            clarity = et.SubElement(ratings, \"clarity\")\n",
      "            clarity.set('count', 'x')\n",
      "            clarity.text = '0'\n",
      "            webrating = et.SubElement(ratings, \"webrating\")\n",
      "            webrating.set('count', sound[ci['num_ratings']])\n",
      "            webrating.text = sound[ci['avg_rating']]\n",
      "            \n",
      "\n",
      "    \n",
      "    # add raw tags\n",
      "    #for sound in raw:\n",
      "    #    if 'raw_keywords' not in sounds[sound[ci['id']]]:\n",
      "    #        sounds[sound[ci['id']]]['raw_keywords'] = sound[ci['keywords']]\n",
      "    print 'processed',filename\n",
      "\n",
      "\n",
      "tree = et.ElementTree(xml)\n",
      "tree.write('3-results/results.xml',pretty_print=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2]\n",
        "processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2-1-150-f680552.csv\n",
        "processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1-1-100-f669064.csv\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "       \n",
      "    print sounds\n",
      "    \n",
      "    sound = et.SubElement(xml, \"sound\")\n",
      "    \n",
      "    w = open('3-results-csv/'+filename, 'wb')\n",
      "    reader = unicodecsv.reader(f)\n",
      "    wr = unicodecsv.writer(w)\n",
      "\n",
      "    # write column names\n",
      "    wr.writerow(reader.next())\n",
      "    \n",
      "    \n",
      "    \n",
      "    # for each worker get the platformWorkerId\n",
      "    for worker in workers:\n",
      "        print workers[worker]\n",
      "    \n",
      "                                       'preview-hq-ogg':sound[ci['preview-hq-ogg']],\n",
      "                                       'preview-hq-mp3':sound[ci['preview-hq-mp3']],\n",
      "            \n",
      "    \n",
      "ET.SubElement(doc, \"field1\", name=\"blah\").text = \"some value1\"\n",
      "ET.SubElement(doc, \"field2\", name=\"asdfasd\").text = \"some vlaue2\"\n",
      "\n",
      "tree = ET.ElementTree(root)\n",
      "tree.write(\"filename.xml\")\n",
      "        \n",
      "    print \"Processed\",filename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "<soundcollection>\n",
      "    <sound id=\"95939\" duration=\"0.4\" url=\"dldlldkdjdk\" title=\"Titel van het geluid\">\n",
      "        <ratings>\n",
      "            <clarity value=\"5\" provenance=\"\"></clarity>\n",
      "        \n",
      "        </ratings>\n",
      "        <description>\n",
      "            Beschrijving van het geluid\n",
      "        </description>\n",
      "        <author-tags>\n",
      "            <tag label=\"bla\"></tag>\n",
      "            \n",
      "        </author-tags>\n",
      "        <crowd-tags>\n",
      "            <tag label=\"bla bla\" count=\"10\" children=\"2\">\n",
      "                <raw label=\"bla bla\" count=\"6\"></raw>\n",
      "                <raw label=\"bla\" count=\"4\"></raw>\n",
      "            </tag>\n",
      "        </crowd-tags>\n",
      "    </sound>\n",
      "\n",
      "</soundcollection>"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}